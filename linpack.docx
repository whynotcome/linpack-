1.安装编译环境：

2.安装mpich
下载mpich软件包，然后执行以下命令：
	解压：tar zxvf mpich-3.2.1.tar.gz
	进入mpich-3.2.1文件目录下输入安装命令：
		./configure --prefix=/usr/local/mpich（ = 后面是文件安装路径）
	完成之后输入make&make install完成安装。
然后向环境变量添加该软件的库路径：
	在.bashrc文件末尾添加以下命令：
		export PATH=/usr/local/linpack/mpich/bin:$PATH
		export  LD_LIBRARY_PATH=/usr/local/linpack/mpich/lib:$BRARY_PATH

测试mpich是否安装成功：
	输入which mpirun会显示安装路径：
	进入mpich解压目录下，进入examples目录，编译hellow.c文件，然后执行查看结果与文件内容描述是否相符，输出结果应为下图：
	
3.安装openblas
（1）下载openblas.tar.gz，openblas是基于Gotoblas优化的库文件。
（2）解压缩。
（3）修改Openblas解压目录下的Makefile.rule文件。
	去掉以下内容的注释：
		version=0.3.9
		Fc=gfortran   #使用Fortran编译器
		BINARY=64   #64位系统
		USE_OPENMP=1  #多线程
（4）运行make，安装库文件。
	成功安装会出现下图：

然后根据提示全部安装：
	
4.安装hpl
（1）下载hpl-2.3.tar.gz
（2）解压，进入setup目录下复制Make.Linux_PII_FBLAS，放到上层目录。
（3）修改该复制文件。具体参数修改如下：



完成后输入make arch=Linux_PII_FBLAS，然后我们在hpl安装目录下bin目录下会看到生成HPL.dat和xhpl文件，表示我们已经安装成功。
5.hpl测试
	进入HPL.dat所在目录下，输入命令：mpirun -np 4 ./xhpl执行xhpl文件会使用默认的HPL.dat文件进行测试，测试结果如下：

（1）通过汇总并处理测试所得数据，形成以下图像：







计算数据单位为Gflops；













	从上图可以看出，随着N的增大，计算速度也随之线性增长，随着NB的增大，计算速度也增大；由此估计，性能是随着矩阵规模N和分块大小NB增大而增大。
	接下来我们直接将N增大到500  600 700 800，NB增大到50 60 70 80，观察效果。






由上图可以得出，并不是一味地增大N和NB就可以得到更好的性能，一定存在一个区间使得当前计算机的测试性能最好。
	（2）同样的，我们猜测二位处理器网格P×Q对计算性能也会有影响，例如对于4个进程的测试，P×Q的值为1*4和2*2会有性能的差异。
	采用控制变量的思想，固定N=29 30 34 35，NB=1 2 3 4，测试PQ=1*4的数据，并与2*2的测试结果进行比较，得到数据结果如下：

	蓝色表示P×Q=1*4，红色表示P×Q=2*2，可以明显的看出前者性能比后者好很多。
综上两点，我们可以推测N、NB在一个较合适的范围内并且P×Q的值满足P=2^n&P≤Q的情况下，计算性能会有很大提高。至于N、NB在那个范围内取值较为合适，在接下来的优化中会详细介绍。

6.优化并进行测试
	首先我们要知道HPL.dat文件中各个参数对应的含义：

	在上面直接测试过程中，我们已经发现N、NB、P、Q等参数对计算性能有较大影响，因此对于优化方向，主要是对相关参数进行调整。调整的主要思想还是以控制变量为主。

（1）矩阵规模N越大，有效计算所占比例也会越大，系统浮点处理性能也就越高；但同时，N的增加会导致内存消耗量的增加，一旦系统实际内存空间不足，性能会大幅降低，因此，要在尽量增大矩阵规模的同时，保证不超出内存限制，经验得出：矩阵占用系统内存的80%左右最佳，即N×N×8=总内存×80%，通过该公式计算出本机所适用的N在500-600之间。
（2）NB：分块的大小对性能有很大的影响，其选择主要通过实际测试得到最优值；NB不能太大或太小，一般在256以下，NB×8一定是Cache line的倍数。查阅相关资料，intel下采用BLAS数学库，一般NB的经验取值为192；

	对于以上两点，我们大致估计N与NB的主要取值范围为
	N=500 520 540 560 580
	NB=64 128 192 256	

（3）P、Q的要求：
		P×Q=进程数=CPU数；
		P≤Q，P值尽量取小，因为列向通信量（通信次数和通信数据量）要远大于横向通信，在上面测试过程中，可以明显发现1*2、1*4、1*8、1*16的计算速度要比2*2、2*4、4*4要快很多。
		P=2^n，HPL中，L分解的列向量通信采用二元交换法，当列向处理器个数P为2的幂时，性能最优。

	对于P、Q的硬性要求和可变要求，相应于不同的进程数，我们做出以下取值：
		单进程：P=1   Q=1
		2个进程：P=1   Q=2
		4个进程：P=1   Q=4
		8个进程：P=1   Q=8         
		16个进程：P=1   Q=16          P=4   Q=4
然后进行测试，分组测试结果如下：
(1)P*Q=1*1
部分测试截图：

结果数据整理并做出图像：


	由图像可以得出，整体上来说，对于P×Q=1×1，N=580，NB=64时，计算性能较好。

(2)P×Q=1×2
参数修改：
	
测试数据如下：
	
	总体来说，NB=128性能较好，但是在N=540时，计算性能大幅降低，推测原因是内存被占用太多，导致计算速度变慢，后面又增大，可能是用到了缓存/cache，改变了存储空间，计算性能就又提高了，最后又降低，表明整个内存已经快被占满，导致计算速度降低。

(3)P×Q=1×4

	NB=192/256整体要比NB=64/128要高，并且在N=540，NB=256时达到最高，之后降低，原因为内存不够，导致计算速度降低。
	在之前测试中已经得到，P×Q=1*4要比2*2整体性能要高，在此就不赘述。

(4)P×Q=1×8





整体呈上升趋势，NB越大，性能越好，但是NB再变大会影响性能，因为NB表示矩阵分块的大小，虽然矩阵分解的越小越细对求解有帮助，但是太小会导致内存消耗变大，从而使性能降低。并且大多数经验已经表明，NB≤256最好。

(5)P×Q=1×16
	

	N=540，NB=256时Gflops最大，然后降低，同样的内存容量问题。
这里对于P×Q=4*4，仅仅选择一组测试数据足以说明P×Q=1*16的优势：





N=520

综上对HPL.dat中相关参数的调整，我们得到了相应的测试数据以及性能变化图像，可以直观的看出随参数变化，计算速度的变化。
	由此，我们可以总结出HPL的优化方案：
	1.N在较小范围内一直增大，计算性能会一直提高，但是如果超出了一个限制，计算性能反而会降低，因此我们可以根据测试数据和相关资料最终确定不同进程数下N的值。
	2.NB的变化也是基于大量的测试数据得出的经验结论，但是和N一样，不能过大或太小，一般在256以下，且NB×8是Cache line 的倍数。虽然NB的选择还与其他因素有关，例如网络、通信方式等，但仅限于机器与实验工具，我们只通过测试数据来估计。
	3.P×Q的取值：
	显而易见，对P的取值要尽量小且为2^n，这是由于矩阵处理过程中列向的通信量要远大于横向通信量、矩阵分解中列向通信采用的是二元交换法（该参数在HPL.dat中可以修改，但是通过查阅资料发现二元交换法性能最优）；
	HPL硬性要求：P×Q=进程数。
	所以我们尽量将P×Q的取值取成1×进程数。
	4.其他参数优化，其他参数的改变对于不同的机器有不同的设置值，实验过程中通过该网址：
	https://www.advancedclustering.com/act_kb/tune-hpl-dat-file/
	可以快捷得到本机器的参数设置。例如我的参数设置如下：
	
	虽然其他参数对测试会有影响，但总体影响不大，主要因素还是N、NB、P、Q四个参数。
7.数据整理，结果图表如下：



